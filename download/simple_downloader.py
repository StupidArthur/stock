# encoding: utf-8

"""
ç²¾ç®€ç‰ˆè‚¡ç¥¨æ•°æ®ä¸‹è½½å™¨
ä»old_codeé‡æ„è€Œæ¥ï¼Œä¸“æ³¨äºæ ¸å¿ƒä¸‹è½½åŠŸèƒ½

ä½¿ç”¨æ–¹æ³•:
1. å®Œæ•´ä¸‹è½½: python data_downloader.py
   - ä¸‹è½½æ‰€æœ‰è‚¡ç¥¨çš„å†å²æ•°æ®ï¼ˆä»2023å¹´å¼€å§‹åˆ°ä»Šå¤©ï¼‰
   - ä¸‹è½½æˆåŠŸåä¼šæ¸…ç†æ—§æ•°æ®ï¼Œåªä¿ç•™å½“å¤©çš„æ•°æ®
   
2. å¢é‡æ›´æ–°: python data_downloader.py update
   - æ£€æŸ¥æœ¬åœ°æœ€æ–°æ•°æ®æ—¥æœŸ
   - åªä¸‹è½½ç¼ºå¤±çš„æœ€æ–°æ•°æ®ï¼ˆæŒ‰å·¥ä½œæ—¥è®¡ç®—ï¼‰
   - æ›´æ–°æˆåŠŸåä¿ç•™æœ€è¿‘3å¤©çš„æ•°æ®
   - é€‚åˆæ—¥å¸¸æ•°æ®æ›´æ–°ä½¿ç”¨
"""

import time
import pandas as pd
from datetime import datetime, timedelta
from pathlib import Path
import tushare as ts
from typing import List, Optional
import threading
import numpy as np
import shutil

# åˆå§‹åŒ–tushare
TOKEN = '0bb5c055c25ec5ccb72bac19aef5440e181aec495d80f03e92a358c1'
ts.set_token(TOKEN)
TUSHARE_API = ts.pro_api()

# æ•°æ®å­˜å‚¨è·¯å¾„
BASE_DATA_DIR = Path(__file__).parent.parent / "data"
STOCK_LIST_FILE = Path(__file__).parent.parent / "stock_list.csv"

# APIé¢‘ç‡é™åˆ¶é…ç½®
API_CALLS_PER_MINUTE = 40
MIN_CALL_INTERVAL = 60.0 / API_CALLS_PER_MINUTE  # æ¯æ¬¡è°ƒç”¨æœ€å°é—´éš”ï¼ˆ1.5ç§’ï¼‰


class APIRateLimiter:
    """APIé¢‘ç‡é™åˆ¶å™¨"""
    def __init__(self, calls_per_minute=API_CALLS_PER_MINUTE):
        self.min_interval = 60.0 / calls_per_minute
        self.last_call_time = 0
        self.call_count = 0
        self.lock = threading.Lock()
    
    def wait_if_needed(self):
        """å¦‚æœéœ€è¦ï¼Œç­‰å¾…åˆ°ä¸‹æ¬¡å¯ä»¥è°ƒç”¨çš„æ—¶é—´"""
        with self.lock:
            current_time = time.time()
            time_since_last_call = current_time - self.last_call_time
            
            if time_since_last_call < self.min_interval:
                sleep_time = self.min_interval - time_since_last_call
                print(f"â³ APIé¢‘ç‡é™åˆ¶ï¼šç­‰å¾… {sleep_time:.2f} ç§’")
                time.sleep(sleep_time)
            
            self.last_call_time = time.time()
            self.call_count += 1
            if self.call_count % 10 == 0:
                print(f"ğŸ“Š å·²è°ƒç”¨API {self.call_count} æ¬¡")


class StockDataDownloader:
    """è‚¡ç¥¨æ•°æ®ä¸‹è½½å™¨"""
    
    def __init__(self, target_date: str = None):
        """
        åˆå§‹åŒ–ä¸‹è½½å™¨
        Args:
            target_date: ç›®æ ‡æ—¥æœŸï¼Œæ ¼å¼YYYYMMDDï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨å½“å‰æ—¥æœŸ
        """
        if target_date is None:
            target_date = datetime.now().strftime("%Y%m%d")
        
        self.target_date = target_date
        self.data_dir = BASE_DATA_DIR / target_date
        self.data_dir.mkdir(parents=True, exist_ok=True)
        
        # åŠ è½½è‚¡ç¥¨åˆ—è¡¨
        self.stocks_df = pd.read_csv(STOCK_LIST_FILE)
        self.rate_limiter = APIRateLimiter()
        
        print(f"ä¸‹è½½å™¨åˆå§‹åŒ–å®Œæˆï¼Œç›®æ ‡æ—¥æœŸ: {target_date}")
        print(f"æ•°æ®ä¿å­˜ç›®å½•: {self.data_dir}")
    
    def _calculate_kdj(self, data: pd.DataFrame) -> pd.DataFrame:
        """è®¡ç®—KDJæŒ‡æ ‡"""
        n = 9
        data = data.sort_values('trade_date', ascending=True).reset_index(drop=True)
        
        close_prices = data['close'].values
        high_prices = data['high'].values
        low_prices = data['low'].values
        
        k_list, d_list, j_list = [], [], []
        k_previous, d_previous = 50, 50
        
        for i in range(len(close_prices)):
            # è®¡ç®—RSVå€¼
            low_min = np.min(low_prices[max(0, i - n + 1):i + 1])
            high_max = np.max(high_prices[max(0, i - n + 1):i + 1])
            rsv = (close_prices[i] - low_min) / (high_max - low_min) * 100
            
            # è®¡ç®—Kå€¼
            k = 2 / 3 * k_previous + 1 / 3 * rsv
            k_list.append(k)
            
            # è®¡ç®—Då€¼
            d = 2 / 3 * d_previous + 1 / 3 * k
            d_list.append(d)
            
            # è®¡ç®—Jå€¼
            j = 3 * k - 2 * d
            j_list.append(j)
            
            k_previous, d_previous = k, d
        
        data['K'] = k_list
        data['D'] = d_list
        data['J'] = j_list
        return data
    
    def _calculate_bbi(self, data: pd.DataFrame) -> pd.DataFrame:
        """è®¡ç®—BBIæŒ‡æ ‡"""
        data['SMA5'] = data['close'].rolling(window=5).mean()
        data['SMA10'] = data['close'].rolling(window=10).mean()
        data['SMA20'] = data['close'].rolling(window=20).mean()
        data['SMA60'] = data['close'].rolling(window=60).mean()
        data['BBI'] = (data['SMA5'] + data['SMA10'] + data['SMA20'] + data['SMA60']) / 4
        return data
    
    def _calculate_white(self, data: pd.DataFrame) -> pd.DataFrame:
        """è®¡ç®—çŸ¥è¡ŒçŸ­æœŸè¶‹åŠ¿çº¿ (white)ï¼šEMA(EMA(C,10),10)"""
        # ç¬¬ä¸€å±‚ï¼šå¯¹æ”¶ç›˜ä»·è®¡ç®—10æ—¥EMA
        data['EMA10'] = data['close'].ewm(span=10, adjust=False).mean()
        # ç¬¬äºŒå±‚ï¼šå¯¹EMA10å†è®¡ç®—10æ—¥EMA
        data['white'] = data['EMA10'].ewm(span=10, adjust=False).mean()
        # åˆ é™¤ä¸­é—´å˜é‡
        data = data.drop('EMA10', axis=1)
        return data
    
    def _calculate_yellow(self, data: pd.DataFrame, m1=5, m2=10, m3=20, m4=60) -> pd.DataFrame:
        """
        è®¡ç®—çŸ¥è¡Œå¤šç©ºçº¿ (yellow)ï¼š(MA(CLOSE,M1)+MA(CLOSE,M2)+MA(CLOSE,M3)+MA(CLOSE,M4))/4
        Args:
            data: è‚¡ç¥¨æ•°æ®
            m1, m2, m3, m4: å››ä¸ªç§»åŠ¨å¹³å‡çº¿çš„å‘¨æœŸï¼Œé»˜è®¤ä¸º5, 10, 20, 60
        """
        ma1 = data['close'].rolling(window=m1).mean()
        ma2 = data['close'].rolling(window=m2).mean()
        ma3 = data['close'].rolling(window=m3).mean()
        ma4 = data['close'].rolling(window=m4).mean()
        
        # è®¡ç®—å››ä¸ªMAçš„å¹³å‡å€¼
        data['yellow'] = (ma1 + ma2 + ma3 + ma4) / 4
        return data
    
    def download_stock_batch(self, stock_codes: List[str], start_date: str, end_date: str) -> pd.DataFrame:
        """
        æ‰¹é‡ä¸‹è½½è‚¡ç¥¨æ•°æ®
        Args:
            stock_codes: è‚¡ç¥¨ä»£ç åˆ—è¡¨
            start_date: å¼€å§‹æ—¥æœŸ
            end_date: ç»“æŸæ—¥æœŸ
        Returns:
            ä¸‹è½½çš„æ•°æ®DataFrame
        """
        batch_size = 8  # æ¯æ‰¹8åªè‚¡ç¥¨
        all_data = []
        
        print(f"å¼€å§‹ä¸‹è½½ {len(stock_codes)} åªè‚¡ç¥¨çš„æ•°æ®...")
        
        for i in range(0, len(stock_codes), batch_size):
            batch_codes = stock_codes[i:i+batch_size]
            codes_str = ",".join(batch_codes)
            
            print(f"æ­£åœ¨ä¸‹è½½ç¬¬ {i//batch_size + 1} æ‰¹æ•°æ® ({len(batch_codes)} åªè‚¡ç¥¨)...")
            
            try:
                # APIé¢‘ç‡æ§åˆ¶
                self.rate_limiter.wait_if_needed()
                
                # è·å–æ•°æ®
                data = TUSHARE_API.daily(ts_code=codes_str, start_date=start_date, end_date=end_date)
                
                if not data.empty:
                    all_data.append(data)
                    print(f"ç¬¬ {i//batch_size + 1} æ‰¹æˆåŠŸè·å– {len(data)} æ¡è®°å½•")
                else:
                    print(f"ç¬¬ {i//batch_size + 1} æ‰¹æ²¡æœ‰æ•°æ®")
                    
            except Exception as e:
                print(f"ç¬¬ {i//batch_size + 1} æ‰¹ä¸‹è½½å¤±è´¥: {e}")
                continue
        
        if all_data:
            combined_data = pd.concat(all_data, ignore_index=True)
            print(f"âœ… æ‰¹é‡ä¸‹è½½å®Œæˆï¼Œæ€»å…±è·å–äº† {len(combined_data)} æ¡æ•°æ®è®°å½•")
            return combined_data
        else:
            print("âŒ æ²¡æœ‰æˆåŠŸè·å–åˆ°ä»»ä½•æ•°æ®")
            return pd.DataFrame()
    
    def process_and_save_data(self, data: pd.DataFrame):
        """å¤„ç†æ•°æ®å¹¶ä¿å­˜åˆ°æœ¬åœ°æ–‡ä»¶"""
        if data.empty:
            print("æ²¡æœ‰æ•°æ®éœ€è¦å¤„ç†")
            return
        
        # æŒ‰è‚¡ç¥¨ä»£ç åˆ†ç»„å¤„ç†
        grouped = data.groupby('ts_code')
        saved_count = 0
        
        for ts_code, group_data in grouped:
            try:
                # è®¡ç®—æŠ€æœ¯æŒ‡æ ‡
                processed_data = self._calculate_kdj(group_data.copy())
                processed_data = self._calculate_bbi(processed_data)
                processed_data = self._calculate_white(processed_data)
                processed_data = self._calculate_yellow(processed_data)
                
                # æ–‡ä»¶åä½¿ç”¨è‚¡ç¥¨ä»£ç çš„å‰6ä½æ•°å­—
                file_name = f"{ts_code[:6]}.parquet"
                file_path = self.data_dir / file_name
                
                # å¦‚æœæ–‡ä»¶å·²å­˜åœ¨ï¼Œéœ€è¦åˆå¹¶æ•°æ®
                if file_path.exists():
                    existing_data = pd.read_parquet(file_path)
                    
                    # åªåˆå¹¶åŸºç¡€äº¤æ˜“æ•°æ®ï¼Œä¸åŒ…å«æŠ€æœ¯æŒ‡æ ‡
                    base_columns = ['ts_code', 'trade_date', 'open', 'high', 'low', 'close', 
                                  'pre_close', 'change', 'pct_chg', 'vol', 'amount']
                    
                    # ç¡®ä¿æ–°æ•°æ®åªåŒ…å«åŸºç¡€åˆ—
                    new_base_data = group_data[base_columns].copy()
                    existing_base_data = existing_data[base_columns].copy()
                    
                    # åˆå¹¶åŸºç¡€æ•°æ®å¹¶å»é‡
                    combined_base_data = pd.concat([existing_base_data, new_base_data])
                    combined_base_data = combined_base_data.drop_duplicates(subset=['trade_date']).sort_values('trade_date')
                    
                    # ä¸ºåˆå¹¶åçš„å®Œæ•´æ•°æ®é‡æ–°è®¡ç®—æŠ€æœ¯æŒ‡æ ‡
                    final_data = self._calculate_kdj(combined_base_data)
                    final_data = self._calculate_bbi(final_data)
                    final_data = self._calculate_white(final_data)
                    final_data = self._calculate_yellow(final_data)
                    
                    final_data.to_parquet(file_path, index=False)
                else:
                    processed_data.to_parquet(file_path, index=False)
                
                saved_count += 1
                print(f"å·²ä¿å­˜ {ts_code} çš„æ•°æ®åˆ° {file_name}")
                
            except Exception as e:
                print(f"å¤„ç† {ts_code} æ•°æ®æ—¶å‡ºé”™: {e}")
                continue
        
        print(f"æˆåŠŸä¿å­˜äº† {saved_count} åªè‚¡ç¥¨çš„æ•°æ®")
    
    def cleanup_old_data(self):
        """
        æ¸…ç†æ—§çš„æ•°æ®ç›®å½•ï¼Œåªä¿ç•™å½“å¤©çš„æ•°æ®
        """
        if not BASE_DATA_DIR.exists():
            return
        
        print(f"\nğŸ§¹ å¼€å§‹æ¸…ç†æ—§æ•°æ®ï¼Œåªä¿ç•™å½“å¤©æ•°æ® ({self.target_date})...")
        
        # è·å–æ‰€æœ‰æ—¥æœŸç›®å½•
        date_dirs = []
        for item in BASE_DATA_DIR.iterdir():
            if item.is_dir() and item.name.isdigit() and len(item.name) == 8:
                try:
                    # éªŒè¯æ˜¯å¦ä¸ºæœ‰æ•ˆæ—¥æœŸ
                    datetime.strptime(item.name, "%Y%m%d")
                    date_dirs.append(item)
                except ValueError:
                    continue
        
        # æ‰¾å‡ºéœ€è¦åˆ é™¤çš„ç›®å½•ï¼ˆæ‰€æœ‰éå½“å¤©çš„ç›®å½•ï¼‰
        dirs_to_delete = [d for d in date_dirs if d.name != self.target_date]
        
        if not dirs_to_delete:
            print(f"æ²¡æœ‰å‘ç°æ—§æ•°æ®ç›®å½•ï¼Œå½“å‰åªæœ‰ä»Šå¤©çš„æ•°æ® ({self.target_date})")
            return
        
        print(f"ä¿ç•™ç›®å½•: {self.target_date}")
        print(f"å°†åˆ é™¤ç›®å½•: {[d.name for d in dirs_to_delete]}")
        
        deleted_count = 0
        total_size_freed = 0
        
        for dir_to_delete in dirs_to_delete:
            try:
                # è®¡ç®—ç›®å½•å¤§å°
                dir_size = sum(f.stat().st_size for f in dir_to_delete.rglob('*') if f.is_file())
                
                # åˆ é™¤ç›®å½•
                shutil.rmtree(dir_to_delete)
                deleted_count += 1
                total_size_freed += dir_size
                
                print(f"âœ… å·²åˆ é™¤ç›®å½•: {dir_to_delete.name} (å¤§å°: {dir_size/1024/1024:.1f}MB)")
                
            except Exception as e:
                print(f"âŒ åˆ é™¤ç›®å½• {dir_to_delete.name} å¤±è´¥: {e}")
        
        if deleted_count > 0:
            print(f"\nğŸ¯ æ¸…ç†å®Œæˆ:")
            print(f"   åˆ é™¤äº† {deleted_count} ä¸ªæ—§æ•°æ®ç›®å½•")
            print(f"   é‡Šæ”¾ç£ç›˜ç©ºé—´: {total_size_freed/1024/1024:.1f}MB")
            print(f"   ç°åœ¨åªä¿ç•™å½“å¤©æ•°æ®: {self.target_date}")
        else:
            print("\nâŒ æ²¡æœ‰æˆåŠŸåˆ é™¤ä»»ä½•ç›®å½•")
    
    def verify_download_success(self) -> bool:
        """
        éªŒè¯ä»Šå¤©çš„æ•°æ®æ˜¯å¦ä¸‹è½½æˆåŠŸ
        Returns:
            True if successful, False otherwise
        """
        if not self.data_dir.exists():
            return False
        
        # æ£€æŸ¥parquetæ–‡ä»¶æ•°é‡
        parquet_files = list(self.data_dir.glob("*.parquet"))
        expected_count = len(self.stocks_df[self.stocks_df['market'].isin(['ä¸»æ¿', 'åˆ›ä¸šæ¿'])])
        
        success_rate = len(parquet_files) / expected_count if expected_count > 0 else 0
        
        print(f"\nğŸ“Š ä¸‹è½½éªŒè¯:")
        print(f"   é¢„æœŸæ–‡ä»¶æ•°: {expected_count}")
        print(f"   å®é™…æ–‡ä»¶æ•°: {len(parquet_files)}")
        print(f"   æˆåŠŸç‡: {success_rate*100:.1f}%")
        
        # å¦‚æœæˆåŠŸç‡è¶…è¿‡95%ï¼Œè®¤ä¸ºä¸‹è½½æˆåŠŸ
        return success_rate >= 0.95
    
    def download_all_stocks(self, start_date: str = "20230101", end_date: str = None):
        """
        ä¸‹è½½æ‰€æœ‰ä¸»æ¿å’Œåˆ›ä¸šæ¿è‚¡ç¥¨æ•°æ®
        Args:
            start_date: å¼€å§‹æ—¥æœŸï¼Œé»˜è®¤"20230101"
            end_date: ç»“æŸæ—¥æœŸï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨target_date
        """
        if end_date is None:
            end_date = self.target_date
        
        print(f"=== è‚¡ç¥¨æ•°æ®ä¸‹è½½å™¨ ===")
        print(f"ä¸‹è½½èŒƒå›´: {start_date} åˆ° {end_date}")
        print(f"è‚¡ç¥¨èŒƒå›´: ä¸»æ¿å’Œåˆ›ä¸šæ¿æ‰€æœ‰è‚¡ç¥¨")
        print(f"ä¿å­˜ç›®å½•: {self.data_dir}")
        print("ğŸš¦ APIé¢‘ç‡æ§åˆ¶: æ¯åˆ†é’Ÿæœ€å¤š40æ¬¡è°ƒç”¨ï¼ˆæ¯1.5ç§’ä¸€æ¬¡ï¼‰")
        
        start_time = time.time()
        
        # è·å–æ‰€æœ‰ä¸»æ¿å’Œåˆ›ä¸šæ¿è‚¡ç¥¨ä»£ç 
        main_stocks = self.stocks_df[self.stocks_df['market'].isin(['ä¸»æ¿', 'åˆ›ä¸šæ¿'])]
        all_codes = main_stocks['ts_code'].tolist()
        
        print(f"éœ€è¦ä¸‹è½½ {len(all_codes)} åªè‚¡ç¥¨çš„æ•°æ®")
        
        # ä¸‹è½½æ•°æ®
        data = self.download_stock_batch(all_codes, start_date, end_date)
        
        # å¤„ç†å¹¶ä¿å­˜æ•°æ®
        self.process_and_save_data(data)
        
        end_time = time.time()
        duration = end_time - start_time
        print(f"\n=== ä¸‹è½½å®Œæˆ ===")
        print(f"æ€»è¿è¡Œæ—¶é—´: {duration:.2f} ç§’ ({duration/60:.2f} åˆ†é’Ÿ)")
        print(f"æ•°æ®å·²ä¿å­˜åˆ°: {self.data_dir}")
        print("æ¯åªè‚¡ç¥¨çš„æ•°æ®ä¿å­˜ä¸ºå•ç‹¬çš„parquetæ–‡ä»¶ï¼Œæ–‡ä»¶åä¸ºè‚¡ç¥¨ä»£ç å‰6ä½.parquet")
        
        # éªŒè¯ä¸‹è½½æ˜¯å¦æˆåŠŸï¼Œå¦‚æœæˆåŠŸåˆ™æ¸…ç†æ—§æ•°æ®
        if self.verify_download_success():
            print(f"\nâœ… ä»Šæ—¥æ•°æ®ä¸‹è½½æˆåŠŸï¼å¼€å§‹æ¸…ç†æ—§æ•°æ®...")
            self.cleanup_old_data()  # åªä¿ç•™å½“å¤©çš„æ•°æ®
        else:
            print(f"\nâš ï¸ ä»Šæ—¥æ•°æ®ä¸‹è½½å¯èƒ½ä¸å®Œæ•´ï¼Œè·³è¿‡æ¸…ç†æ—§æ•°æ®")


def download_today_data():
    """ä¸‹è½½ä»Šå¤©çš„æ•°æ® - ä¸»ç¨‹åºå…¥å£"""
    today = datetime.now().strftime("%Y%m%d")
    downloader = StockDataDownloader(target_date=today)
    downloader.download_all_stocks()


if __name__ == "__main__":
    download_today_data()
    