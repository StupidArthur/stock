# encoding: utf-8

"""
ç²¾ç®€ç‰ˆè‚¡ç¥¨æ•°æ®ä¸‹è½½å™¨
ä»old_codeé‡æ„è€Œæ¥ï¼Œä¸“æ³¨äºæ ¸å¿ƒä¸‹è½½åŠŸèƒ½

ä½¿ç”¨æ–¹æ³•:
1. å®Œæ•´ä¸‹è½½: python data_downloader.py
   - ä¸‹è½½æ‰€æœ‰è‚¡ç¥¨çš„å†å²æ•°æ®ï¼ˆä»2023å¹´å¼€å§‹åˆ°ä»Šå¤©ï¼‰
   - ä¸‹è½½æˆåŠŸåä¼šæ¸…ç†æ—§æ•°æ®ï¼Œåªä¿ç•™å½“å¤©çš„æ•°æ®
   
2. å¢é‡æ›´æ–°: python data_downloader.py update
   - æ£€æŸ¥æœ¬åœ°æœ€æ–°æ•°æ®æ—¥æœŸ
   - åªä¸‹è½½ç¼ºå¤±çš„æœ€æ–°æ•°æ®ï¼ˆæŒ‰å·¥ä½œæ—¥è®¡ç®—ï¼‰
   - æ›´æ–°æˆåŠŸåä¿ç•™æœ€è¿‘3å¤©çš„æ•°æ®
   - é€‚åˆæ—¥å¸¸æ•°æ®æ›´æ–°ä½¿ç”¨
"""

import time
import pandas as pd
from datetime import datetime, timedelta
from pathlib import Path
import tushare as ts
from typing import List, Optional
import threading
import numpy as np
import shutil

# åˆå§‹åŒ–tushare
TOKEN = '0bb5c055c25ec5ccb72bac19aef5440e181aec495d80f03e92a358c1'
ts.set_token(TOKEN)
TUSHARE_API = ts.pro_api()

# æ•°æ®å­˜å‚¨è·¯å¾„
BASE_DATA_DIR = Path(__file__).parent.parent / "data"
STOCK_LIST_FILE = Path(__file__).parent.parent / "stock_list.csv"

# APIé¢‘ç‡é™åˆ¶é…ç½®
API_CALLS_PER_MINUTE = 40
MIN_CALL_INTERVAL = 60.0 / API_CALLS_PER_MINUTE  # æ¯æ¬¡è°ƒç”¨æœ€å°é—´éš”ï¼ˆ1.5ç§’ï¼‰


class APIRateLimiter:
    """APIé¢‘ç‡é™åˆ¶å™¨"""
    def __init__(self, calls_per_minute=API_CALLS_PER_MINUTE):
        self.min_interval = 60.0 / calls_per_minute
        self.last_call_time = 0
        self.call_count = 0
        self.lock = threading.Lock()
    
    def wait_if_needed(self):
        """å¦‚æœéœ€è¦ï¼Œç­‰å¾…åˆ°ä¸‹æ¬¡å¯ä»¥è°ƒç”¨çš„æ—¶é—´"""
        with self.lock:
            current_time = time.time()
            time_since_last_call = current_time - self.last_call_time
            
            if time_since_last_call < self.min_interval:
                sleep_time = self.min_interval - time_since_last_call
                print(f"â³ APIé¢‘ç‡é™åˆ¶ï¼šç­‰å¾… {sleep_time:.2f} ç§’")
                time.sleep(sleep_time)
            
            self.last_call_time = time.time()
            self.call_count += 1
            if self.call_count % 10 == 0:
                print(f"ğŸ“Š å·²è°ƒç”¨API {self.call_count} æ¬¡")


class StockDataDownloader:
    """è‚¡ç¥¨æ•°æ®ä¸‹è½½å™¨"""
    
    def __init__(self, target_date: str = None):
        """
        åˆå§‹åŒ–ä¸‹è½½å™¨
        Args:
            target_date: ç›®æ ‡æ—¥æœŸï¼Œæ ¼å¼YYYYMMDDï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨å½“å‰æ—¥æœŸ
        """
        if target_date is None:
            target_date = datetime.now().strftime("%Y%m%d")
        
        self.target_date = target_date
        self.data_dir = BASE_DATA_DIR / target_date
        self.data_dir.mkdir(parents=True, exist_ok=True)
        
        # åŠ è½½è‚¡ç¥¨åˆ—è¡¨
        self.stocks_df = pd.read_csv(STOCK_LIST_FILE)
        self.rate_limiter = APIRateLimiter()
        
        print(f"ä¸‹è½½å™¨åˆå§‹åŒ–å®Œæˆï¼Œç›®æ ‡æ—¥æœŸ: {target_date}")
        print(f"æ•°æ®ä¿å­˜ç›®å½•: {self.data_dir}")
    
    def _calculate_kdj(self, data: pd.DataFrame) -> pd.DataFrame:
        """è®¡ç®—KDJæŒ‡æ ‡"""
        n = 9
        data = data.sort_values('trade_date', ascending=True).reset_index(drop=True)
        
        close_prices = data['close'].values
        high_prices = data['high'].values
        low_prices = data['low'].values
        
        k_list, d_list, j_list = [], [], []
        k_previous, d_previous = 50, 50
        
        for i in range(len(close_prices)):
            # è®¡ç®—RSVå€¼
            low_min = np.min(low_prices[max(0, i - n + 1):i + 1])
            high_max = np.max(high_prices[max(0, i - n + 1):i + 1])
            rsv = (close_prices[i] - low_min) / (high_max - low_min) * 100
            
            # è®¡ç®—Kå€¼
            k = 2 / 3 * k_previous + 1 / 3 * rsv
            k_list.append(k)
            
            # è®¡ç®—Då€¼
            d = 2 / 3 * d_previous + 1 / 3 * k
            d_list.append(d)
            
            # è®¡ç®—Jå€¼
            j = 3 * k - 2 * d
            j_list.append(j)
            
            k_previous, d_previous = k, d
        
        data['K'] = k_list
        data['D'] = d_list
        data['J'] = j_list
        return data
    
    def _calculate_bbi(self, data: pd.DataFrame) -> pd.DataFrame:
        """è®¡ç®—BBIæŒ‡æ ‡"""
        data['SMA5'] = data['close'].rolling(window=5).mean()
        data['SMA10'] = data['close'].rolling(window=10).mean()
        data['SMA20'] = data['close'].rolling(window=20).mean()
        data['SMA60'] = data['close'].rolling(window=60).mean()
        data['BBI'] = (data['SMA5'] + data['SMA10'] + data['SMA20'] + data['SMA60']) / 4
        return data
    
    def download_stock_batch(self, stock_codes: List[str], start_date: str, end_date: str) -> pd.DataFrame:
        """
        æ‰¹é‡ä¸‹è½½è‚¡ç¥¨æ•°æ®
        Args:
            stock_codes: è‚¡ç¥¨ä»£ç åˆ—è¡¨
            start_date: å¼€å§‹æ—¥æœŸ
            end_date: ç»“æŸæ—¥æœŸ
        Returns:
            ä¸‹è½½çš„æ•°æ®DataFrame
        """
        batch_size = 8  # æ¯æ‰¹8åªè‚¡ç¥¨
        all_data = []
        
        print(f"å¼€å§‹ä¸‹è½½ {len(stock_codes)} åªè‚¡ç¥¨çš„æ•°æ®...")
        
        for i in range(0, len(stock_codes), batch_size):
            batch_codes = stock_codes[i:i+batch_size]
            codes_str = ",".join(batch_codes)
            
            print(f"æ­£åœ¨ä¸‹è½½ç¬¬ {i//batch_size + 1} æ‰¹æ•°æ® ({len(batch_codes)} åªè‚¡ç¥¨)...")
            
            try:
                # APIé¢‘ç‡æ§åˆ¶
                self.rate_limiter.wait_if_needed()
                
                # è·å–æ•°æ®
                data = TUSHARE_API.daily(ts_code=codes_str, start_date=start_date, end_date=end_date)
                
                if not data.empty:
                    all_data.append(data)
                    print(f"ç¬¬ {i//batch_size + 1} æ‰¹æˆåŠŸè·å– {len(data)} æ¡è®°å½•")
                else:
                    print(f"ç¬¬ {i//batch_size + 1} æ‰¹æ²¡æœ‰æ•°æ®")
                    
            except Exception as e:
                print(f"ç¬¬ {i//batch_size + 1} æ‰¹ä¸‹è½½å¤±è´¥: {e}")
                continue
        
        if all_data:
            combined_data = pd.concat(all_data, ignore_index=True)
            print(f"âœ… æ‰¹é‡ä¸‹è½½å®Œæˆï¼Œæ€»å…±è·å–äº† {len(combined_data)} æ¡æ•°æ®è®°å½•")
            return combined_data
        else:
            print("âŒ æ²¡æœ‰æˆåŠŸè·å–åˆ°ä»»ä½•æ•°æ®")
            return pd.DataFrame()
    
    def process_and_save_data(self, data: pd.DataFrame):
        """å¤„ç†æ•°æ®å¹¶ä¿å­˜åˆ°æœ¬åœ°æ–‡ä»¶"""
        if data.empty:
            print("æ²¡æœ‰æ•°æ®éœ€è¦å¤„ç†")
            return
        
        # æŒ‰è‚¡ç¥¨ä»£ç åˆ†ç»„å¤„ç†
        grouped = data.groupby('ts_code')
        saved_count = 0
        
        for ts_code, group_data in grouped:
            try:
                # è®¡ç®—æŠ€æœ¯æŒ‡æ ‡
                processed_data = self._calculate_kdj(group_data.copy())
                processed_data = self._calculate_bbi(processed_data)
                
                # æ–‡ä»¶åä½¿ç”¨è‚¡ç¥¨ä»£ç çš„å‰6ä½æ•°å­—
                file_name = f"{ts_code[:6]}.parquet"
                file_path = self.data_dir / file_name
                
                # å¦‚æœæ–‡ä»¶å·²å­˜åœ¨ï¼Œéœ€è¦åˆå¹¶æ•°æ®
                if file_path.exists():
                    existing_data = pd.read_parquet(file_path)
                    
                    # åªåˆå¹¶åŸºç¡€äº¤æ˜“æ•°æ®ï¼Œä¸åŒ…å«æŠ€æœ¯æŒ‡æ ‡
                    base_columns = ['ts_code', 'trade_date', 'open', 'high', 'low', 'close', 
                                  'pre_close', 'change', 'pct_chg', 'vol', 'amount']
                    
                    # ç¡®ä¿æ–°æ•°æ®åªåŒ…å«åŸºç¡€åˆ—
                    new_base_data = group_data[base_columns].copy()
                    existing_base_data = existing_data[base_columns].copy()
                    
                    # åˆå¹¶åŸºç¡€æ•°æ®å¹¶å»é‡
                    combined_base_data = pd.concat([existing_base_data, new_base_data])
                    combined_base_data = combined_base_data.drop_duplicates(subset=['trade_date']).sort_values('trade_date')
                    
                    # ä¸ºåˆå¹¶åçš„å®Œæ•´æ•°æ®é‡æ–°è®¡ç®—æŠ€æœ¯æŒ‡æ ‡
                    final_data = self._calculate_kdj(combined_base_data)
                    final_data = self._calculate_bbi(final_data)
                    
                    final_data.to_parquet(file_path, index=False)
                else:
                    processed_data.to_parquet(file_path, index=False)
                
                saved_count += 1
                print(f"å·²ä¿å­˜ {ts_code} çš„æ•°æ®åˆ° {file_name}")
                
            except Exception as e:
                print(f"å¤„ç† {ts_code} æ•°æ®æ—¶å‡ºé”™: {e}")
                continue
        
        print(f"æˆåŠŸä¿å­˜äº† {saved_count} åªè‚¡ç¥¨çš„æ•°æ®")
    
    def cleanup_old_data(self):
        """
        æ¸…ç†æ—§çš„æ•°æ®ç›®å½•ï¼Œåªä¿ç•™å½“å¤©çš„æ•°æ®
        """
        if not BASE_DATA_DIR.exists():
            return
        
        print(f"\nğŸ§¹ å¼€å§‹æ¸…ç†æ—§æ•°æ®ï¼Œåªä¿ç•™å½“å¤©æ•°æ® ({self.target_date})...")
        
        # è·å–æ‰€æœ‰æ—¥æœŸç›®å½•
        date_dirs = []
        for item in BASE_DATA_DIR.iterdir():
            if item.is_dir() and item.name.isdigit() and len(item.name) == 8:
                try:
                    # éªŒè¯æ˜¯å¦ä¸ºæœ‰æ•ˆæ—¥æœŸ
                    datetime.strptime(item.name, "%Y%m%d")
                    date_dirs.append(item)
                except ValueError:
                    continue
        
        # æ‰¾å‡ºéœ€è¦åˆ é™¤çš„ç›®å½•ï¼ˆæ‰€æœ‰éå½“å¤©çš„ç›®å½•ï¼‰
        dirs_to_delete = [d for d in date_dirs if d.name != self.target_date]
        
        if not dirs_to_delete:
            print(f"æ²¡æœ‰å‘ç°æ—§æ•°æ®ç›®å½•ï¼Œå½“å‰åªæœ‰ä»Šå¤©çš„æ•°æ® ({self.target_date})")
            return
        
        print(f"ä¿ç•™ç›®å½•: {self.target_date}")
        print(f"å°†åˆ é™¤ç›®å½•: {[d.name for d in dirs_to_delete]}")
        
        deleted_count = 0
        total_size_freed = 0
        
        for dir_to_delete in dirs_to_delete:
            try:
                # è®¡ç®—ç›®å½•å¤§å°
                dir_size = sum(f.stat().st_size for f in dir_to_delete.rglob('*') if f.is_file())
                
                # åˆ é™¤ç›®å½•
                shutil.rmtree(dir_to_delete)
                deleted_count += 1
                total_size_freed += dir_size
                
                print(f"âœ… å·²åˆ é™¤ç›®å½•: {dir_to_delete.name} (å¤§å°: {dir_size/1024/1024:.1f}MB)")
                
            except Exception as e:
                print(f"âŒ åˆ é™¤ç›®å½• {dir_to_delete.name} å¤±è´¥: {e}")
        
        if deleted_count > 0:
            print(f"\nğŸ¯ æ¸…ç†å®Œæˆ:")
            print(f"   åˆ é™¤äº† {deleted_count} ä¸ªæ—§æ•°æ®ç›®å½•")
            print(f"   é‡Šæ”¾ç£ç›˜ç©ºé—´: {total_size_freed/1024/1024:.1f}MB")
            print(f"   ç°åœ¨åªä¿ç•™å½“å¤©æ•°æ®: {self.target_date}")
        else:
            print("\nâŒ æ²¡æœ‰æˆåŠŸåˆ é™¤ä»»ä½•ç›®å½•")
    
    def verify_download_success(self) -> bool:
        """
        éªŒè¯ä»Šå¤©çš„æ•°æ®æ˜¯å¦ä¸‹è½½æˆåŠŸ
        Returns:
            True if successful, False otherwise
        """
        if not self.data_dir.exists():
            return False
        
        # æ£€æŸ¥parquetæ–‡ä»¶æ•°é‡
        parquet_files = list(self.data_dir.glob("*.parquet"))
        expected_count = len(self.stocks_df[self.stocks_df['market'].isin(['ä¸»æ¿', 'åˆ›ä¸šæ¿'])])
        
        success_rate = len(parquet_files) / expected_count if expected_count > 0 else 0
        
        print(f"\nğŸ“Š ä¸‹è½½éªŒè¯:")
        print(f"   é¢„æœŸæ–‡ä»¶æ•°: {expected_count}")
        print(f"   å®é™…æ–‡ä»¶æ•°: {len(parquet_files)}")
        print(f"   æˆåŠŸç‡: {success_rate*100:.1f}%")
        
        # å¦‚æœæˆåŠŸç‡è¶…è¿‡95%ï¼Œè®¤ä¸ºä¸‹è½½æˆåŠŸ
        return success_rate >= 0.95
    
    def download_all_stocks(self, start_date: str = "20230101", end_date: str = None):
        """
        ä¸‹è½½æ‰€æœ‰ä¸»æ¿å’Œåˆ›ä¸šæ¿è‚¡ç¥¨æ•°æ®
        Args:
            start_date: å¼€å§‹æ—¥æœŸï¼Œé»˜è®¤"20230101"
            end_date: ç»“æŸæ—¥æœŸï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨target_date
        """
        if end_date is None:
            end_date = self.target_date
        
        print(f"=== è‚¡ç¥¨æ•°æ®ä¸‹è½½å™¨ ===")
        print(f"ä¸‹è½½èŒƒå›´: {start_date} åˆ° {end_date}")
        print(f"è‚¡ç¥¨èŒƒå›´: ä¸»æ¿å’Œåˆ›ä¸šæ¿æ‰€æœ‰è‚¡ç¥¨")
        print(f"ä¿å­˜ç›®å½•: {self.data_dir}")
        print("ğŸš¦ APIé¢‘ç‡æ§åˆ¶: æ¯åˆ†é’Ÿæœ€å¤š40æ¬¡è°ƒç”¨ï¼ˆæ¯1.5ç§’ä¸€æ¬¡ï¼‰")
        
        start_time = time.time()
        
        # è·å–æ‰€æœ‰ä¸»æ¿å’Œåˆ›ä¸šæ¿è‚¡ç¥¨ä»£ç 
        main_stocks = self.stocks_df[self.stocks_df['market'].isin(['ä¸»æ¿', 'åˆ›ä¸šæ¿'])]
        all_codes = main_stocks['ts_code'].tolist()
        
        print(f"éœ€è¦ä¸‹è½½ {len(all_codes)} åªè‚¡ç¥¨çš„æ•°æ®")
        
        # ä¸‹è½½æ•°æ®
        data = self.download_stock_batch(all_codes, start_date, end_date)
        
        # å¤„ç†å¹¶ä¿å­˜æ•°æ®
        self.process_and_save_data(data)
        
        end_time = time.time()
        duration = end_time - start_time
        print(f"\n=== ä¸‹è½½å®Œæˆ ===")
        print(f"æ€»è¿è¡Œæ—¶é—´: {duration:.2f} ç§’ ({duration/60:.2f} åˆ†é’Ÿ)")
        print(f"æ•°æ®å·²ä¿å­˜åˆ°: {self.data_dir}")
        print("æ¯åªè‚¡ç¥¨çš„æ•°æ®ä¿å­˜ä¸ºå•ç‹¬çš„parquetæ–‡ä»¶ï¼Œæ–‡ä»¶åä¸ºè‚¡ç¥¨ä»£ç å‰6ä½.parquet")
        
        # éªŒè¯ä¸‹è½½æ˜¯å¦æˆåŠŸï¼Œå¦‚æœæˆåŠŸåˆ™æ¸…ç†æ—§æ•°æ®
        if self.verify_download_success():
            print(f"\nâœ… ä»Šæ—¥æ•°æ®ä¸‹è½½æˆåŠŸï¼å¼€å§‹æ¸…ç†æ—§æ•°æ®...")
            self.cleanup_old_data()  # åªä¿ç•™å½“å¤©çš„æ•°æ®
        else:
            print(f"\nâš ï¸ ä»Šæ—¥æ•°æ®ä¸‹è½½å¯èƒ½ä¸å®Œæ•´ï¼Œè·³è¿‡æ¸…ç†æ—§æ•°æ®")


def download_today_data():
    """ä¸‹è½½ä»Šå¤©çš„æ•°æ® - ä¸»ç¨‹åºå…¥å£"""
    today = datetime.now().strftime("%Y%m%d")
    downloader = StockDataDownloader(target_date=today)
    downloader.download_all_stocks()

def update_today_data():
    """
    æ›´æ–°ä»Šå¤©çš„æ•°æ® - ä¸»ç¨‹åºå…¥å£
    åŒºåˆ«äºdownload_today_dataï¼Œupdate_today_dataä¼šè¯»å–å½“å‰Dataç›®å½•ä¸‹æœ€æ–°çš„ç›®å½•ï¼Œå¦‚ä»Šå¤©æ˜¯20250903,å‘ç°ç›®å½•ä¸‹å·²ç»æœ‰ä¸€ä¸ª20250902çš„ç›®å½•ã€‚
    å¹¶ä¸”ä»¥000001ä¸ºæŠ½æ ·ï¼Œç¡®è®¤å…¶ç¡®å®æœ‰20250903çš„æ•°æ®ã€‚
    æ­¤æ—¶åªéœ€è¦ä¸‹è½½æ‰€æœ‰è‚¡ç¥¨åœ¨20250903çš„æ•°æ®ï¼Œæ‹¼æ¥æœ¬åœ°æ•°æ®å³å¯ã€‚
    ä½†å› ä¸ºæ¯åˆ†é’Ÿåªèƒ½è°ƒç”¨40æ¬¡æ•°æ®ï¼Œå¦‚æœä¾ç„¶æ˜¯8ä¸ªè‚¡ç¥¨ä¸€æ¬¡è°ƒç”¨ï¼Œå¹¶ä¸ä¼šä½¿å¾—æ•ˆç‡å˜é«˜ã€‚æ‰€ä»¥å¯ä»¥ç›´æ¥æ”¹æˆæ‰€æœ‰è‚¡ç¥¨çš„æ•°æ®ä¸€èµ·æ‹¿ï¼ŒæŒ‰å¤©æ‹¿æ‰€æœ‰è‚¡ç¥¨çš„æ•°æ®ã€‚æ€»å…±è‚¡ç¥¨å¤§æ¦‚åœ¨4000~5000ã€‚ä¸€æ¬¡APIè¯·æ±‚æœ€å¤š6000æ¡ï¼Œç†è®ºä¸Šæ˜¯å¯ä»¥çš„ã€‚
    å¦‚æœæœ€æ–°çš„æ•°æ®æ˜¯20250901çš„ï¼Œé‚£ä¹ˆå°±éœ€è¦è¯·æ±‚2æ¬¡ã€‚
    ç”±äºå¢é‡æ›´æ–°å¯èƒ½å­˜åœ¨é”™è¯¯ï¼Œæ‰€ä»¥è®²å†å²æ•°æ®æ¸…ç†çš„åŠŸèƒ½å˜æˆä¿ç•™3å¤©çš„æ•°æ®ã€‚
    æ”¹åŠ¨ä¸è¦å½±å“åˆ°download_today_dataçš„è¿è¡Œã€‚
    """
    today = datetime.now().strftime("%Y%m%d")
    
    # æ‰¾åˆ°æœ€æ–°çš„æ•°æ®ç›®å½•
    latest_date = _find_latest_data_directory()
    if not latest_date:
        print("âŒ æœªæ‰¾åˆ°ä»»ä½•å†å²æ•°æ®ç›®å½•ï¼Œè¯·å…ˆè¿è¡Œ download_today_data()")
        return
    
    print(f"ğŸ“ æ‰¾åˆ°æœ€æ–°æ•°æ®ç›®å½•: {latest_date}")
    
    # æ£€æŸ¥000001æ ·æœ¬è‚¡ç¥¨çš„æœ€æ–°æ•°æ®æ—¥æœŸ
    sample_latest_date = _check_sample_stock_latest_date(latest_date)
    if not sample_latest_date:
        print(f"âŒ æ— æ³•ä»æ ·æœ¬è‚¡ç¥¨000001è·å–æœ€æ–°æ•°æ®æ—¥æœŸ")
        return
    
    print(f"ğŸ“Š æ ·æœ¬è‚¡ç¥¨000001æœ€æ–°æ•°æ®æ—¥æœŸ: {sample_latest_date}")
    
    # è®¡ç®—éœ€è¦ä¸‹è½½çš„æ—¥æœŸèŒƒå›´
    missing_dates = _get_missing_dates(sample_latest_date, today)
    if not missing_dates:
        print(f"âœ… æ•°æ®å·²æ˜¯æœ€æ–°ï¼Œæ— éœ€æ›´æ–°")
        return
    
    print(f"ğŸ“… éœ€è¦ä¸‹è½½çš„æ—¥æœŸ: {missing_dates}")
    
    # åˆ›å»ºä¸‹è½½å™¨å¹¶æ‰§è¡Œå¢é‡ä¸‹è½½
    downloader = StockDataDownloader(target_date=today)
    
    # è·å–æ‰€æœ‰ä¸»æ¿å’Œåˆ›ä¸šæ¿è‚¡ç¥¨ä»£ç 
    main_stocks = downloader.stocks_df[downloader.stocks_df['market'].isin(['ä¸»æ¿', 'åˆ›ä¸šæ¿'])]
    all_codes = main_stocks['ts_code'].tolist()
    
    print(f"ğŸš€ å¼€å§‹å¢é‡æ›´æ–°ï¼Œè‚¡ç¥¨æ•°é‡: {len(all_codes)}")
    
    # å¦‚æœåªæœ‰ä¸€å¤©çš„æ•°æ®è¦ä¸‹è½½ï¼Œä¸”ä»Šå¤©çš„ç›®å½•ä¸å­˜åœ¨ï¼Œéœ€è¦å…ˆå¤åˆ¶å†å²æ•°æ®
    if len(missing_dates) <= 2 and not downloader.data_dir.exists():
        print(f"ğŸ“‹ æ£€æµ‹åˆ°éœ€è¦åˆå¹¶å†å²æ•°æ®ï¼Œæ­£åœ¨å¤åˆ¶ {latest_date} çš„æ•°æ®åˆ° {today}...")
        _copy_historical_data_to_today(latest_date, today)
    
    # æŒ‰æ—¥æœŸä¸‹è½½æ•°æ®
    for date_str in missing_dates:
        print(f"\nğŸ“ˆ æ­£åœ¨ä¸‹è½½ {date_str} çš„æ•°æ®...")
        data = _download_single_day_all_stocks(downloader, all_codes, date_str)
        if not data.empty:
            downloader.process_and_save_data(data)
            print(f"âœ… {date_str} æ•°æ®ä¸‹è½½å®Œæˆ")
        else:
            print(f"âš ï¸ {date_str} æ²¡æœ‰è·å–åˆ°æ•°æ®")
    
    # éªŒè¯å¹¶æ¸…ç†æ•°æ®ï¼ˆä¿ç•™3å¤©ï¼‰
    if downloader.verify_download_success():
        print(f"\nâœ… å¢é‡æ›´æ–°å®Œæˆï¼å¼€å§‹æ¸…ç†æ—§æ•°æ®ï¼ˆä¿ç•™3å¤©ï¼‰...")
        _cleanup_old_data_keep_3_days(today)
    else:
        print(f"\nâš ï¸ æ•°æ®æ›´æ–°å¯èƒ½ä¸å®Œæ•´ï¼Œè·³è¿‡æ¸…ç†æ—§æ•°æ®")


def _find_latest_data_directory() -> Optional[str]:
    """
    æŸ¥æ‰¾dataç›®å½•ä¸‹æœ€æ–°çš„æ•°æ®ç›®å½•
    Returns:
        æœ€æ–°çš„æ•°æ®ç›®å½•åç§°ï¼ˆYYYYMMDDæ ¼å¼ï¼‰ï¼Œå¦‚æœæ²¡æœ‰æ‰¾åˆ°åˆ™è¿”å›None
    """
    if not BASE_DATA_DIR.exists():
        return None
    
    # è·å–æ‰€æœ‰æ—¥æœŸç›®å½•
    date_dirs = []
    for item in BASE_DATA_DIR.iterdir():
        if item.is_dir() and item.name.isdigit() and len(item.name) == 8:
            try:
                # éªŒè¯æ˜¯å¦ä¸ºæœ‰æ•ˆæ—¥æœŸ
                datetime.strptime(item.name, "%Y%m%d")
                date_dirs.append(item.name)
            except ValueError:
                continue
    
    if not date_dirs:
        return None
    
    # è¿”å›æœ€æ–°çš„æ—¥æœŸ
    return max(date_dirs)


def _check_sample_stock_latest_date(data_dir_name: str) -> Optional[str]:
    """
    æ£€æŸ¥æ ·æœ¬è‚¡ç¥¨000001çš„æœ€æ–°æ•°æ®æ—¥æœŸ
    Args:
        data_dir_name: æ•°æ®ç›®å½•åç§°
    Returns:
        æ ·æœ¬è‚¡ç¥¨çš„æœ€æ–°æ•°æ®æ—¥æœŸï¼ˆYYYYMMDDæ ¼å¼ï¼‰ï¼Œå¦‚æœæ²¡æœ‰æ‰¾åˆ°åˆ™è¿”å›None
    """
    sample_file = BASE_DATA_DIR / data_dir_name / "000001.parquet"
    if not sample_file.exists():
        return None
    
    try:
        # è¯»å–æ ·æœ¬è‚¡ç¥¨æ•°æ®
        data = pd.read_parquet(sample_file)
        if data.empty:
            return None
        
        # è·å–æœ€æ–°äº¤æ˜“æ—¥æœŸ
        latest_date = data['trade_date'].max()
        return latest_date
        
    except Exception as e:
        print(f"è¯»å–æ ·æœ¬è‚¡ç¥¨æ•°æ®æ—¶å‡ºé”™: {e}")
        return None


def _get_missing_dates(latest_date: str, today: str) -> List[str]:
    """
    è·å–éœ€è¦ä¸‹è½½çš„ç¼ºå¤±æ—¥æœŸåˆ—è¡¨
    Args:
        latest_date: æœ€æ–°æ•°æ®æ—¥æœŸ
        today: ä»Šå¤©æ—¥æœŸ
    Returns:
        éœ€è¦ä¸‹è½½çš„æ—¥æœŸåˆ—è¡¨
    """
    start_date = datetime.strptime(latest_date, "%Y%m%d")
    end_date = datetime.strptime(today, "%Y%m%d")
    
    missing_dates = []
    current_date = start_date + timedelta(days=1)  # ä»ä¸‹ä¸€å¤©å¼€å§‹
    
    while current_date <= end_date:
        # åªè€ƒè™‘å·¥ä½œæ—¥ï¼ˆå‘¨ä¸€åˆ°å‘¨äº”ï¼‰
        if current_date.weekday() < 5:
            missing_dates.append(current_date.strftime("%Y%m%d"))
        current_date += timedelta(days=1)
    
    return missing_dates


def _download_single_day_all_stocks(downloader: StockDataDownloader, stock_codes: List[str], date_str: str) -> pd.DataFrame:
    """
    ä¸‹è½½æŒ‡å®šæ—¥æœŸæ‰€æœ‰è‚¡ç¥¨çš„æ•°æ®
    Args:
        downloader: ä¸‹è½½å™¨å®ä¾‹
        stock_codes: è‚¡ç¥¨ä»£ç åˆ—è¡¨
        date_str: æ—¥æœŸå­—ç¬¦ä¸²ï¼ˆYYYYMMDDæ ¼å¼ï¼‰
    Returns:
        ä¸‹è½½çš„æ•°æ®DataFrame
    """
    # æ¯æ¬¡APIè°ƒç”¨æœ€å¤š6000æ¡è®°å½•ï¼ŒæŒ‰åˆ†æ‰¹å¤„ç†
    batch_size = 1000  # æ¯æ‰¹1000åªè‚¡ç¥¨ï¼Œç¡®ä¿ä¸è¶…è¿‡6000æ¡é™åˆ¶
    all_data = []
    
    print(f"ğŸ“Š å…±éœ€ä¸‹è½½ {len(stock_codes)} åªè‚¡ç¥¨åœ¨ {date_str} çš„æ•°æ®")
    
    for i in range(0, len(stock_codes), batch_size):
        batch_codes = stock_codes[i:i+batch_size]
        codes_str = ",".join(batch_codes)
        
        print(f"   æ­£åœ¨ä¸‹è½½ç¬¬ {i//batch_size + 1} æ‰¹æ•°æ® ({len(batch_codes)} åªè‚¡ç¥¨)...")
        
        try:
            # APIé¢‘ç‡æ§åˆ¶
            downloader.rate_limiter.wait_if_needed()
            
            # è·å–æŒ‡å®šæ—¥æœŸçš„æ•°æ®
            data = TUSHARE_API.daily(ts_code=codes_str, trade_date=date_str)
            
            if not data.empty:
                all_data.append(data)
                print(f"   ç¬¬ {i//batch_size + 1} æ‰¹æˆåŠŸè·å– {len(data)} æ¡è®°å½•")
            else:
                print(f"   ç¬¬ {i//batch_size + 1} æ‰¹æ²¡æœ‰æ•°æ®")
                
        except Exception as e:
            print(f"   ç¬¬ {i//batch_size + 1} æ‰¹ä¸‹è½½å¤±è´¥: {e}")
            continue
    
    if all_data:
        combined_data = pd.concat(all_data, ignore_index=True)
        print(f"âœ… {date_str} æ•°æ®ä¸‹è½½å®Œæˆï¼Œæ€»å…±è·å–äº† {len(combined_data)} æ¡è®°å½•")
        return combined_data
    else:
        print(f"âŒ {date_str} æ²¡æœ‰æˆåŠŸè·å–åˆ°ä»»ä½•æ•°æ®")
        return pd.DataFrame()


def _copy_historical_data_to_today(source_date: str, target_date: str):
    """
    å¤åˆ¶å†å²æ•°æ®åˆ°ä»Šå¤©çš„ç›®å½•ï¼Œä¸ºå¢é‡æ›´æ–°åšå‡†å¤‡
    Args:
        source_date: æºæ•°æ®æ—¥æœŸï¼ˆYYYYMMDDæ ¼å¼ï¼‰
        target_date: ç›®æ ‡æ—¥æœŸï¼ˆYYYYMMDDæ ¼å¼ï¼‰
    """
    source_dir = BASE_DATA_DIR / source_date
    target_dir = BASE_DATA_DIR / target_date
    
    if not source_dir.exists():
        print(f"âŒ æºæ•°æ®ç›®å½•ä¸å­˜åœ¨: {source_dir}")
        return
    
    if target_dir.exists():
        print(f"âš ï¸ ç›®æ ‡ç›®å½•å·²å­˜åœ¨: {target_dir}")
        return
    
    print(f"ğŸ“ åˆ›å»ºç›®æ ‡ç›®å½•: {target_dir}")
    target_dir.mkdir(parents=True, exist_ok=True)
    
    # è·å–æºç›®å½•ä¸­çš„æ‰€æœ‰parquetæ–‡ä»¶
    source_files = list(source_dir.glob("*.parquet"))
    print(f"ğŸ“‹ éœ€è¦å¤åˆ¶ {len(source_files)} ä¸ªæ•°æ®æ–‡ä»¶...")
    
    copied_count = 0
    total_size = 0
    
    for source_file in source_files:
        try:
            target_file = target_dir / source_file.name
            
            # å¤åˆ¶æ–‡ä»¶
            shutil.copy2(source_file, target_file)
            copied_count += 1
            total_size += source_file.stat().st_size
            
            if copied_count % 500 == 0:
                print(f"   å·²å¤åˆ¶ {copied_count}/{len(source_files)} ä¸ªæ–‡ä»¶...")
                
        except Exception as e:
            print(f"âŒ å¤åˆ¶æ–‡ä»¶ {source_file.name} å¤±è´¥: {e}")
    
    print(f"âœ… å†å²æ•°æ®å¤åˆ¶å®Œæˆ:")
    print(f"   æˆåŠŸå¤åˆ¶ {copied_count}/{len(source_files)} ä¸ªæ–‡ä»¶")
    print(f"   æ€»å¤§å°: {total_size/1024/1024:.1f}MB")
    print(f"   ä» {source_date} å¤åˆ¶åˆ° {target_date}")


def _cleanup_old_data_keep_3_days(current_date: str):
    """
    æ¸…ç†æ—§æ•°æ®ï¼Œä¿ç•™3å¤©çš„æ•°æ®
    Args:
        current_date: å½“å‰æ—¥æœŸï¼ˆYYYYMMDDæ ¼å¼ï¼‰
    """
    if not BASE_DATA_DIR.exists():
        return
    
    print(f"\nğŸ§¹ å¼€å§‹æ¸…ç†æ—§æ•°æ®ï¼Œä¿ç•™3å¤©æ•°æ®...")
    
    # è®¡ç®—ä¿ç•™çš„æ—¥æœŸèŒƒå›´ï¼ˆå½“å‰æ—¥æœŸå¾€å‰3å¤©ï¼‰
    current_dt = datetime.strptime(current_date, "%Y%m%d")
    keep_dates = set()
    
    for i in range(3):
        keep_date = current_dt - timedelta(days=i)
        keep_dates.add(keep_date.strftime("%Y%m%d"))
    
    print(f"ä¿ç•™æ—¥æœŸ: {sorted(keep_dates)}")
    
    # è·å–æ‰€æœ‰æ—¥æœŸç›®å½•
    date_dirs = []
    for item in BASE_DATA_DIR.iterdir():
        if item.is_dir() and item.name.isdigit() and len(item.name) == 8:
            try:
                # éªŒè¯æ˜¯å¦ä¸ºæœ‰æ•ˆæ—¥æœŸ
                datetime.strptime(item.name, "%Y%m%d")
                date_dirs.append(item)
            except ValueError:
                continue
    
    # æ‰¾å‡ºéœ€è¦åˆ é™¤çš„ç›®å½•
    dirs_to_delete = [d for d in date_dirs if d.name not in keep_dates]
    
    if not dirs_to_delete:
        print(f"æ²¡æœ‰å‘ç°éœ€è¦åˆ é™¤çš„æ—§æ•°æ®ç›®å½•")
        return
    
    print(f"å°†åˆ é™¤ç›®å½•: {[d.name for d in dirs_to_delete]}")
    
    deleted_count = 0
    total_size_freed = 0
    
    for dir_to_delete in dirs_to_delete:
        try:
            # è®¡ç®—ç›®å½•å¤§å°
            dir_size = sum(f.stat().st_size for f in dir_to_delete.rglob('*') if f.is_file())
            
            # åˆ é™¤ç›®å½•
            shutil.rmtree(dir_to_delete)
            deleted_count += 1
            total_size_freed += dir_size
            
            print(f"âœ… å·²åˆ é™¤ç›®å½•: {dir_to_delete.name} (å¤§å°: {dir_size/1024/1024:.1f}MB)")
            
        except Exception as e:
            print(f"âŒ åˆ é™¤ç›®å½• {dir_to_delete.name} å¤±è´¥: {e}")
    
    if deleted_count > 0:
        print(f"\nğŸ¯ æ¸…ç†å®Œæˆ:")
        print(f"   åˆ é™¤äº† {deleted_count} ä¸ªæ—§æ•°æ®ç›®å½•")
        print(f"   é‡Šæ”¾ç£ç›˜ç©ºé—´: {total_size_freed/1024/1024:.1f}MB")
        print(f"   ç°åœ¨ä¿ç•™æœ€è¿‘3å¤©çš„æ•°æ®")
    else:
        print("\nâŒ æ²¡æœ‰æˆåŠŸåˆ é™¤ä»»ä½•ç›®å½•")


if __name__ == "__main__":
    import sys
    
    if len(sys.argv) > 1 and sys.argv[1] == "update":
        # è¿è¡Œå¢é‡æ›´æ–°
        update_today_data()
    else:
        # é»˜è®¤è¿è¡Œå®Œæ•´ä¸‹è½½
        download_today_data()
